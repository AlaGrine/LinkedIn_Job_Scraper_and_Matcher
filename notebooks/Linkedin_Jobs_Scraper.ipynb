{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a58b7c5d-3d28-4191-a8e9-afeae07bc990",
   "metadata": {},
   "source": [
    "# Scrape LinkedIn Using Selenium, Request and Beautiful Soup in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d968b2-f09e-4df6-924c-4e604c2cac9a",
   "metadata": {},
   "source": [
    "We are going to scrape Linkedin Jobs. More specifically, the following details will be scraped:\n",
    "- Job Id\n",
    "- Job title\n",
    "- Seniority Level\n",
    "- Location\n",
    "- Job description\n",
    "- number of candidats\n",
    "- posted time ago\n",
    "\n",
    "1. To scrape Job Ids, we will use `selenium` to navigate to this URL: `https://www.linkedin.com/jobs/search?`.\n",
    "\n",
    "`chromedriver` executable and your LinkedIn credentials are required here.\n",
    "\n",
    "3. As explained [here](https://www.scrapingdog.com/blog/scrape-linkedin-jobs/), to scrape other details (level, description...), we will use a simple GET request (leveraging the `requests` library) to this URL: `https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/xxxxxx` where xxxxxx is the job ID.\\\n",
    "This is easier than using clicks from `selenium`.\n",
    "\n",
    "Note that I tried to scrape Job IDs using the guest URL `https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?` but the results were imprecise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b212a74-160b-43d2-b109-e1ffb83f1818",
   "metadata": {},
   "source": [
    "# I-Scraping Linkedin Jobs IDs using selenium and BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2618dbb1-7afd-4cb7-9343-7a709fa0f4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install selenium \n",
    "# pip install beautifulsoup4\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import requests\n",
    "\n",
    "import time, datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math, re, sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d21562-d227-4c5b-bd16-093c740036fb",
   "metadata": {},
   "source": [
    "## Login to Linkedin using selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d4139a-49c9-4e01-822e-bad65b123305",
   "metadata": {},
   "source": [
    "üîë **Note:**\n",
    "\n",
    "1. To use `selenium`, we need a web driver. For instance, the `chromedriver` can be downloaded from [here](https://chromedriver.chromium.org/downloads).\\\n",
    "   Next, we add the chromedriver to the project directory.\n",
    "\n",
    "3. Linkedin credentials (email address and password) are also required. You can save them here: `../data/user_credentials.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "283a8218-3229-418f-8239-46dcf7b5fb09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('email_address', 'password')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get User Credentials\n",
    "with open('../data/user_credentials.txt', 'r',encoding=\"utf-8\") as file:\n",
    "    user_credentials = file.readlines()\n",
    "    user_credentials = [line.rstrip() for line in user_credentials]\n",
    "    \n",
    "my_email,my_pwd = user_credentials[0],user_credentials[1]\n",
    "my_email,my_pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1d333e-a704-4b40-be5a-e573e16a4d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Instanciate the chrome service\n",
    "chromedriver_path = '../chromedriver/chromedriver.exe'\n",
    "service = Service(executable_path=chromedriver_path)\n",
    "\n",
    "# 2. Instanciate the webdriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "driver = webdriver.Chrome(options=options, service=service)\n",
    "\n",
    "# 3. Open the LinkedIn login page\n",
    "driver.get('https://www.linkedin.com/login')\n",
    "time.sleep(5) # waiting for the page to load\n",
    "\n",
    "# 4. Enter email address & password\n",
    "email_input = driver.find_element(By.ID, 'username')\n",
    "password_input = driver.find_element(By.ID, 'password')\n",
    "email_input.send_keys(my_email)\n",
    "password_input.send_keys(my_pwd)\n",
    "\n",
    "# 5. Click the login button\n",
    "password_input.send_keys(Keys.ENTER)\n",
    "\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce703cef-b5d7-430d-9d47-c9b2a84587b3",
   "metadata": {},
   "source": [
    "We will be logged into LinkedIn after running the above code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba38bd7-5d4a-451e-9989-c31a924247fa",
   "metadata": {},
   "source": [
    "## Scraping Linkedin Jobs IDs\n",
    "\n",
    "1. Set the search query parameters: keywords (ie. Job title) and location;\n",
    "2. Search results are displayed on many pages: `25` jobs are listed on each page;\n",
    "3. We will navigate to every page using the `start` parameter (0,25,50...);\n",
    "4. We need to scroll to the bottom of the page to load the full data;\n",
    "5. To get Job Ids, we will parse the HTML content of the page using BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2940fc82-b529-43a0-acbf-f967935e37ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "List_Job_IDs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45f8f663-3dbc-4b90-a8db-e084ddb89921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function 'Scroll to the bottom'. \n",
    "\n",
    "# time.sleep() function is used to provide extra time for the webpage to load. \n",
    "# I used 120 seconds. If the 25 jobs have not loaded during this period, we can make adjust it and test again.\n",
    "\n",
    "def scroll_to_bottom(driver,sleep_time=120):\n",
    "    last_height = driver.execute_script('return document.body.scrollHeight')\n",
    "    while True:\n",
    "        driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "        new_height = driver.execute_script('return document.body.scrollHeight')\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "    \n",
    "    time.sleep(sleep_time)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba66ae2e-6aa7-4ac8-a002-16f3236a34bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to the first page (start=0) and scroll to the bottom of the page\n",
    "\n",
    "keywords = 'data%20scientist'\n",
    "location = 'Montreal%2C%20Quebec%2C%20Canada'\n",
    "start = 0\n",
    "\n",
    "url = f'https://www.linkedin.com/jobs/search/?keywords={keywords}&location={location}&start={start}'\n",
    "url = requests.utils.requote_uri(url)\n",
    "driver.get(url)\n",
    "scroll_to_bottom(driver,sleep_time=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4721bbff-4d51-4444-af37-92b243c37890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_of_jobs: 260\n",
      "number_of_pages: 11\n"
     ]
    }
   ],
   "source": [
    "# Get number of jobs found and number of pages:\n",
    "\n",
    "# Parse the HTML content of the page using BeautifulSoup.\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "try:\n",
    "    div_number_of_jobs = soup.find(\"div\",{\"class\":\"jobs-search-results-list__subtitle\"})\n",
    "    number_of_jobs = int(div_number_of_jobs.find('span').get_text().strip().split()[0])\n",
    "except:\n",
    "    number_of_jobs = 0\n",
    "    \n",
    "number_of_pages=math.ceil(number_of_jobs/25)\n",
    "print(\"number_of_jobs:\",number_of_jobs)\n",
    "print(\"number_of_pages:\",number_of_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a990a0c-2cd9-4321-9b5e-43f909e97399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Job Ids present on the first page.\n",
    "\n",
    "def find_Job_Ids(soup):\n",
    "\n",
    "    Job_Ids_on_the_page = []\n",
    "    \n",
    "    job_postings = soup.find_all('li', {'class': 'jobs-search-results__list-item'})\n",
    "    for job_posting in job_postings:\n",
    "        Job_ID = job_posting.get('data-occludable-job-id')\n",
    "        Job_Ids_on_the_page.append(Job_ID)\n",
    "        # job_title = job_posting.find('a', class_='job-card-list__title').get_text().strip()\n",
    "        # location = job_posting.find('li', class_='job-card-container__metadata-item').get_text().strip()\n",
    "    \n",
    "    return Job_Ids_on_the_page    \n",
    "\n",
    "Jobs_on_this_page = find_Job_Ids(soup)\n",
    "List_Job_IDs.extend(Jobs_on_this_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92700a0-ba39-4c40-aa7c-567c0c7e7886",
   "metadata": {},
   "source": [
    "Now that we've scraped the job IDs and number of results from the first page, let's iterate over the remaining pages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e294f1b6-af94-4b96-8e12-698304673a71",
   "metadata": {},
   "source": [
    "### Iterate over the remaining pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f753a25-fcce-403f-8cca-366fdc197406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page: 1...Jobs found:25\n",
      "Scraping page: 2...Jobs found:25\n",
      "Scraping page: 3...Jobs found:25\n",
      "Scraping page: 4...Jobs found:25\n",
      "Scraping page: 5...Jobs found:25\n",
      "Scraping page: 6...Jobs found:25\n",
      "Scraping page: 7...Jobs found:25\n",
      "Scraping page: 8...Jobs found:25\n",
      "Scraping page: 9...Jobs found:25\n",
      "Scraping page: 10...Jobs found:8\n"
     ]
    }
   ],
   "source": [
    "if number_of_pages>1:\n",
    "    \n",
    "    for page_num in range(1,number_of_pages):\n",
    "        print(f\"Scraping page: {page_num}\",end=\"...\")\n",
    "        \n",
    "        # Navigate to page\n",
    "        url = f'https://www.linkedin.com/jobs/search/?keywords={job_title}&location={location}&start={25 * page_num}'\n",
    "        url = requests.utils.requote_uri(url)\n",
    "        driver.get(url)\n",
    "        scroll_to_bottom(driver)\n",
    "\n",
    "        # Parse the HTML content of the page using BeautifulSoup.\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # Get Job Ids present on the page.\n",
    "        Jobs_on_this_page = find_Job_Ids(soup)\n",
    "        List_Job_IDs.extend(Jobs_on_this_page)  \n",
    "        print(f'Jobs found:{len(Jobs_on_this_page)}')\n",
    "\n",
    "pd.DataFrame({\"Job_Id\":List_Job_IDs}).to_csv('../data/Job_Ids.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "632e56da-ef58-4fda-a244-3fe55546f957",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Close the browser and shut down the ChromiumDriver executable that\n",
    "# is started when starting the ChromiumDriver. \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffac401-0e45-46d7-b672-3bc069c9db4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15515da8-558b-4e31-a8a8-2bfbb70f32d8",
   "metadata": {},
   "source": [
    "## Scraping Job description using requests and BeautifulSoup\n",
    "https://www.scrapingdog.com/blog/scrape-linkedin-jobs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fc78f82-7c4a-42fc-aaf3-bc18a08b4910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "list_job_IDs = pd.read_csv(\"../data/Job_Ids.csv\").Job_Id.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5bfbf246-1c5e-403b-b2c2-f40f7a9d49ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3768445795, 3766874608, 3636842773, 3765556140, 3743029150]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_job_IDs = list_job_IDs[:5]\n",
    "list_job_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5556de4c-cb2f-450f-9137-354bb3f589f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags(html):\n",
    "    '''remove html tags from BeautifulSoup.text'''\n",
    " \n",
    "    # parse html content\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    " \n",
    "    for data in soup(['style', 'script']):\n",
    "        # Remove tags\n",
    "        data.decompose()\n",
    " \n",
    "    # return data by retrieving the tag content\n",
    "    return ' '.join(soup.stripped_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "51cdbe4a-4213-4e0c-8017-c180e1eb24b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ... read jobId:3768445795\n",
      "2 ... read jobId:3766874608\n",
      "3 ... read jobId:3636842773\n",
      "4 ... read jobId:3765556140\n",
      "5 ... read jobId:3743029150\n",
      "6 ... read jobId:3765912486\n",
      "7 ... read jobId:3755769264\n",
      "8 ... read jobId:3738828224\n",
      "9 ... read jobId:3717903353\n",
      "10 ... read jobId:3766879037\n",
      "11 ... read jobId:3765709395\n",
      "12 ... read jobId:3676014327\n",
      "13 ... read jobId:3771807918\n",
      "14 ... read jobId:3768668734\n",
      "15 ... read jobId:3770793104\n",
      "16 ... read jobId:3768182240\n",
      "17 ... read jobId:3768141217\n",
      "18 ... read jobId:3713157521\n",
      "19 ... read jobId:3771365625\n",
      "20 ... read jobId:3765392730\n",
      "21 ... read jobId:3645689495\n",
      "22 ... read jobId:3759367245\n",
      "23 ... read jobId:3772194033\n",
      "24 ... read jobId:3764345835\n",
      "25 ... read jobId:3711769725\n",
      "26 ... read jobId:3706108079\n",
      "27 ... read jobId:3762359345\n",
      "28 ... read jobId:3709558547\n",
      "29 ... read jobId:3764677184\n",
      "30 ... read jobId:3759706595\n",
      "31 ... read jobId:3717081904\n",
      "32 ... read jobId:3684313985\n",
      "33 ... read jobId:3760886556\n",
      "34 ... read jobId:3737836614\n",
      "35 ... read jobId:3760588018\n",
      "36 ... read jobId:3730066007\n",
      "37 ... read jobId:3730568203\n",
      "38 ... read jobId:3764380116\n",
      "39 ... read jobId:3601080128\n",
      "40 ... read jobId:3743666116\n",
      "41 ... read jobId:3763586945\n",
      "42 ... read jobId:3729324553\n",
      "43 ... read jobId:3754667556\n",
      "44 ... read jobId:3731724406\n",
      "45 ... read jobId:3764347808\n",
      "46 ... read jobId:3749763511\n",
      "47 ... read jobId:3670994681\n",
      "48 ... read jobId:3719404843\n",
      "49 ... read jobId:3771870806\n",
      "50 ... read jobId:3719407356\n",
      "51 ... read jobId:3760319735\n",
      "52 ... read jobId:3761918497\n",
      "53 ... read jobId:3758739937\n",
      "54 ... read jobId:3701155205\n",
      "55 ... read jobId:3690854165\n",
      "56 ... read jobId:3770561295\n",
      "57 ... read jobId:3673093699\n",
      "58 ... read jobId:3764376281\n",
      "59 ... read jobId:3603372664\n",
      "60 ... read jobId:3729321853\n",
      "61 ... read jobId:3742711599\n",
      "62 ... read jobId:3665635100\n",
      "63 ... read jobId:3772069899\n",
      "64 ... read jobId:3732179688\n",
      "65 ... read jobId:3708449900\n",
      "66 ... read jobId:3771875435\n",
      "67 ... read jobId:3756921213\n",
      "68 ... read jobId:3731217727\n",
      "69 ... read jobId:3756921357\n",
      "70 ... read jobId:3756917465\n",
      "71 ... read jobId:3597329466\n",
      "72 ... read jobId:3772184596\n",
      "73 ... read jobId:3756922759\n",
      "74 ... read jobId:3601077478\n",
      "75 ... read jobId:3755732049\n",
      "76 ... read jobId:3673085377\n",
      "77 ... read jobId:3699066823\n",
      "78 ... read jobId:3681459889\n",
      "79 ... read jobId:3724676562\n",
      "80 ... read jobId:3736667425\n",
      "81 ... read jobId:3693023268\n",
      "82 ... read jobId:3755366086\n",
      "83 ... read jobId:3763548658\n",
      "84 ... read jobId:3750107971\n",
      "85 ... read jobId:3659394315\n",
      "86 ... read jobId:3742685304\n",
      "87 ... read jobId:3689358485\n",
      "88 ... read jobId:3738313239\n",
      "89 ... read jobId:3735172683\n",
      "90 ... read jobId:3765161499\n",
      "91 ... read jobId:3765366759\n",
      "92 ... read jobId:3756916657\n",
      "93 ... read jobId:3772178910\n",
      "94 ... read jobId:3756919913\n",
      "95 ... read jobId:3658376761\n",
      "96 ... read jobId:3756921798\n",
      "97 ... read jobId:3772184223\n",
      "98 ... read jobId:3756920798\n",
      "99 ... read jobId:3755588550\n",
      "100 ... read jobId:3732393084\n",
      "101 ... read jobId:3599187301\n",
      "102 ... read jobId:3748001259\n",
      "103 ... read jobId:3674202734\n",
      "104 ... read jobId:3674764925\n",
      "105 ... read jobId:3765994973\n",
      "106 ... read jobId:3733396936\n",
      "107 ... read jobId:3750890542\n",
      "108 ... read jobId:3762800628\n",
      "109 ... read jobId:3735171706\n",
      "110 ... read jobId:3770557267\n",
      "111 ... read jobId:3751172880\n",
      "112 ... read jobId:3641771854\n",
      "113 ... read jobId:3655208781\n",
      "114 ... read jobId:3744502669\n",
      "115 ... read jobId:3767690782\n",
      "116 ... read jobId:3752393934\n",
      "117 ... read jobId:3738310399\n",
      "118 ... read jobId:3772182747\n",
      "119 ... read jobId:3765160542\n",
      "120 ... read jobId:3767381183\n",
      "121 ... read jobId:3756921294\n",
      "122 ... read jobId:3756917424\n",
      "123 ... read jobId:3767308516\n",
      "124 ... read jobId:3756922333\n",
      "125 ... read jobId:3761785263\n",
      "126 ... read jobId:3772181730\n",
      "127 ... read jobId:3770759699\n",
      "128 ... read jobId:3726994742\n",
      "129 ... read jobId:3623107594\n",
      "130 ... read jobId:3766176256\n",
      "131 ... read jobId:3762362371\n",
      "132 ... read jobId:3721792560\n",
      "133 ... read jobId:3763550600\n",
      "134 ... read jobId:3765382573\n",
      "135 ... read jobId:3771649742\n",
      "136 ... read jobId:3669545462\n",
      "137 ... read jobId:3757044524\n",
      "138 ... read jobId:3689957639\n",
      "139 ... read jobId:3679649797\n",
      "140 ... read jobId:3678779721\n",
      "141 ... read jobId:3603376330\n",
      "142 ... read jobId:3760419851\n",
      "143 ... read jobId:3737206813\n",
      "144 ... read jobId:3757725140\n",
      "145 ... read jobId:3749027996\n",
      "146 ... read jobId:3767690752\n",
      "147 ... read jobId:3756921317\n",
      "148 ... read jobId:3756916550\n",
      "149 ... read jobId:3756915625\n",
      "150 ... read jobId:3765155938\n",
      "151 ... read jobId:3744436272\n",
      "152 ... read jobId:3746512498\n",
      "153 ... read jobId:3762791917\n",
      "154 ... read jobId:3750345972\n",
      "155 ... read jobId:3708436900\n",
      "156 ... read jobId:3744478689\n",
      "157 ... read jobId:3765034867\n",
      "158 ... read jobId:3771871758\n",
      "159 ... read jobId:3764041924\n",
      "160 ... read jobId:3738045068\n",
      "161 ... read jobId:3637106219\n",
      "162 ... read jobId:3771876439\n",
      "163 ... read jobId:3767682789\n",
      "164 ... read jobId:3743669314\n",
      "165 ... read jobId:3771877165\n",
      "166 ... read jobId:3748621358\n",
      "167 ... read jobId:3766044223\n",
      "168 ... read jobId:3767650227\n",
      "169 ... read jobId:3772032623\n",
      "170 ... read jobId:3765151432\n",
      "171 ... read jobId:3770781869\n",
      "172 ... read jobId:3765139139\n",
      "173 ... read jobId:3765157670\n",
      "174 ... read jobId:3772082642\n",
      "175 ... read jobId:3767650188\n",
      "176 ... read jobId:3757661694\n",
      "177 ... read jobId:3750345959\n",
      "178 ... read jobId:3757665074\n",
      "179 ... read jobId:3765162467\n",
      "180 ... read jobId:3757300794\n",
      "181 ... read jobId:3716407253\n",
      "182 ... read jobId:3754364390\n",
      "183 ... read jobId:3772173361\n",
      "184 ... read jobId:3767679377\n",
      "185 ... read jobId:3767684634\n",
      "186 ... read jobId:3752364405\n",
      "187 ... read jobId:3610572925\n",
      "188 ... read jobId:3741486290\n",
      "189 ... read jobId:3765716596\n",
      "190 ... read jobId:3750036546\n",
      "191 ... read jobId:3758728150\n",
      "192 ... read jobId:3761825344\n",
      "193 ... read jobId:3760383939\n",
      "194 ... read jobId:3768661968\n",
      "195 ... read jobId:3765158602\n",
      "196 ... read jobId:3772301672\n",
      "197 ... read jobId:3771871777\n",
      "198 ... read jobId:3771842635\n",
      "199 ... read jobId:3765126732\n",
      "200 ... read jobId:3771899477\n",
      "201 ... read jobId:3735241075\n",
      "202 ... read jobId:3741487310\n",
      "203 ... read jobId:3765140900\n",
      "204 ... read jobId:3762737956\n",
      "205 ... read jobId:3758729314\n",
      "206 ... read jobId:3765156910\n",
      "207 ... read jobId:3764377203\n",
      "208 ... read jobId:3728082224\n",
      "209 ... read jobId:3733084040\n",
      "210 ... read jobId:3759264074\n",
      "211 ... read jobId:3765159551\n",
      "212 ... read jobId:3767634819\n",
      "213 ... read jobId:3771736687\n",
      "214 ... read jobId:3765128738\n",
      "215 ... read jobId:3767630089\n",
      "216 ... read jobId:3765163124\n",
      "217 ... read jobId:3765137457\n",
      "218 ... read jobId:3765161671\n",
      "219 ... read jobId:3747472480\n",
      "220 ... read jobId:3772317255\n",
      "221 ... read jobId:3771888682\n",
      "222 ... read jobId:3772304371\n",
      "223 ... read jobId:3755399158\n",
      "224 ... read jobId:3765160714\n",
      "225 ... read jobId:3767378508\n",
      "226 ... read jobId:3765212177\n",
      "227 ... read jobId:3771734982\n",
      "228 ... read jobId:3756656853\n",
      "229 ... read jobId:3767871219\n",
      "230 ... read jobId:3762695166\n",
      "231 ... read jobId:3765707902\n",
      "232 ... read jobId:3772165077\n",
      "233 ... read jobId:3772311748\n",
      "234 ... read jobId:3772303366\n",
      "235 ... read jobId:3771876438\n",
      "236 ... read jobId:3772301645\n",
      "237 ... read jobId:3767380272\n",
      "238 ... read jobId:3772093222\n",
      "239 ... read jobId:3765156911\n",
      "240 ... read jobId:3765161668\n",
      "241 ... read jobId:3765161505\n",
      "242 ... read jobId:3767697862\n",
      "243 ... read jobId:3767618516\n",
      "244 ... read jobId:3767651160\n",
      "245 ... read jobId:3764368653\n",
      "246 ... read jobId:3772312772\n",
      "247 ... read jobId:3767904136\n",
      "248 ... read jobId:3767622160\n",
      "249 ... read jobId:3765161497\n",
      "250 ... read jobId:3767380310\n",
      "251 ... read jobId:3727346073\n",
      "252 ... read jobId:3772088447\n",
      "253 ... read jobId:3767698342\n",
      "254 ... read jobId:3771993021\n",
      "255 ... read jobId:3768331939\n",
      "256 ... read jobId:3772167880\n",
      "257 ... read jobId:3772305359\n",
      "258 ... read jobId:3772322007\n"
     ]
    }
   ],
   "source": [
    "job_url='https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/{}'\n",
    "job={}\n",
    "list_jobs=[]\n",
    "\n",
    "for j in range(0,len(list_job_IDs)):\n",
    "    print(f\"{j+1} ... read jobId:{list_job_IDs[j]}\")\n",
    "\n",
    "    resp = requests.get(job_url.format(list_job_IDs[j]))\n",
    "    soup=BeautifulSoup(resp.text,'html.parser')\n",
    "    # print(soup.prettify()) \n",
    "\n",
    "    job[\"Job_ID\"] = list_job_IDs[j] \n",
    "    # try:\n",
    "    #     job[\"Job_html\"] = resp.content\n",
    "    # except:\n",
    "    #     job[\"Job_html\"]=None\n",
    "\n",
    "    try: # remove tags\n",
    "        job[\"Job_txt\"] = remove_tags(resp.content)\n",
    "    except:\n",
    "        job[\"Job_txt\"] = None\n",
    "    \n",
    "    try:\n",
    "        job[\"company\"]=soup.find(\"div\",{\"class\":\"top-card-layout__card\"}).find(\"a\").find(\"img\").get('alt')\n",
    "    except:\n",
    "        job[\"company\"]=None\n",
    "\n",
    "    try:\n",
    "        job[\"job-title\"]=soup.find(\"div\",{\"class\":\"top-card-layout__entity-info\"}).find(\"a\").text.strip()\n",
    "    except:\n",
    "        job[\"job-title\"]=None\n",
    "\n",
    "    try:\n",
    "        job[\"level\"]=soup.find(\"ul\",{\"class\":\"description__job-criteria-list\"}).find(\"li\").text.replace(\"Seniority level\",\"\").strip()\n",
    "    except:\n",
    "        job[\"level\"]=None\n",
    "\n",
    "    try:\n",
    "        job[\"location\"]=soup.find(\"span\",{\"class\":\"topcard__flavor topcard__flavor--bullet\"}).text.strip()\n",
    "    except:\n",
    "        job[\"location\"]=None\n",
    "\n",
    "    try:\n",
    "        job[\"posted-time-ago\"]=soup.find(\"span\",{\"class\":\"posted-time-ago__text topcard__flavor--metadata\"}).text.strip()\n",
    "    except:\n",
    "        job[\"posted-time-ago\"]=None\n",
    "\n",
    "    try:\n",
    "        nb_candidats = soup.find(\"span\",{\"class\":\"num-applicants__caption topcard__flavor--metadata topcard__flavor--bullet\"}).text.strip()\n",
    "        nb_candidats = int(nb_candidats.split()[0])\n",
    "        job[\"nb_candidats\"]= nb_candidats\n",
    "    except:\n",
    "        job[\"nb_candidats\"]=None\n",
    "\n",
    "    list_jobs.append(job)\n",
    "    job={}\n",
    "\n",
    "# create a pandas Datadrame\n",
    "jobs_DF = pd.DataFrame(list_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2bffddba-4bb4-463f-a019-d76c56c21983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_ID</th>\n",
       "      <th>Job_txt</th>\n",
       "      <th>company</th>\n",
       "      <th>job-title</th>\n",
       "      <th>level</th>\n",
       "      <th>location</th>\n",
       "      <th>posted-time-ago</th>\n",
       "      <th>nb_candidats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3768445795</td>\n",
       "      <td>Data Scientist / Scientifique des Donn√©es Brai...</td>\n",
       "      <td>BrainFinance</td>\n",
       "      <td>Data Scientist / Scientifique des Donn√©es</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>Montreal, Quebec, Canada</td>\n",
       "      <td>6 days ago</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3766874608</td>\n",
       "      <td>Data Scientist Ubisoft Montreal, Quebec, Canad...</td>\n",
       "      <td>Ubisoft</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>Montreal, Quebec, Canada</td>\n",
       "      <td>None</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3636842773</td>\n",
       "      <td>Data Scientist / Senior Data Scientist StackAd...</td>\n",
       "      <td>StackAdapt</td>\n",
       "      <td>Data Scientist / Senior Data Scientist</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>Canada</td>\n",
       "      <td>1 week ago</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3765556140</td>\n",
       "      <td>Data Scientist / Scientifique des donn√©es McGi...</td>\n",
       "      <td>McGill St Laurent</td>\n",
       "      <td>Data Scientist / Scientifique des donn√©es</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>Montreal, Quebec, Canada</td>\n",
       "      <td>1 week ago</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3743029150</td>\n",
       "      <td>Scientifique en IA Appliqu√© / AI Research Scie...</td>\n",
       "      <td>Thales</td>\n",
       "      <td>Scientifique en IA Appliqu√© / AI Research Scie...</td>\n",
       "      <td>Not Applicable</td>\n",
       "      <td>Montreal, Quebec, Canada</td>\n",
       "      <td>1 week ago</td>\n",
       "      <td>97.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Job_ID                                            Job_txt  \\\n",
       "0  3768445795  Data Scientist / Scientifique des Donn√©es Brai...   \n",
       "1  3766874608  Data Scientist Ubisoft Montreal, Quebec, Canad...   \n",
       "2  3636842773  Data Scientist / Senior Data Scientist StackAd...   \n",
       "3  3765556140  Data Scientist / Scientifique des donn√©es McGi...   \n",
       "4  3743029150  Scientifique en IA Appliqu√© / AI Research Scie...   \n",
       "\n",
       "             company                                          job-title  \\\n",
       "0       BrainFinance          Data Scientist / Scientifique des Donn√©es   \n",
       "1            Ubisoft                                     Data Scientist   \n",
       "2         StackAdapt             Data Scientist / Senior Data Scientist   \n",
       "3  McGill St Laurent          Data Scientist / Scientifique des donn√©es   \n",
       "4             Thales  Scientifique en IA Appliqu√© / AI Research Scie...   \n",
       "\n",
       "              level                  location posted-time-ago  nb_candidats  \n",
       "0  Mid-Senior level  Montreal, Quebec, Canada      6 days ago           NaN  \n",
       "1  Mid-Senior level  Montreal, Quebec, Canada            None          70.0  \n",
       "2  Mid-Senior level                    Canada      1 week ago           NaN  \n",
       "3  Mid-Senior level  Montreal, Quebec, Canada      1 week ago           NaN  \n",
       "4    Not Applicable  Montreal, Quebec, Canada      1 week ago          97.0  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs_DF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a199bfad-5d8b-47cb-83e3-4c8553dfc14a",
   "metadata": {},
   "source": [
    "üîë **Note:**\n",
    "\n",
    "Now we have scraped all Linkedin Job details. \n",
    "\n",
    "The next step is to process the data:\n",
    "1. Create a posted_date column using posted_time_ago;\n",
    "2. Clean up 'Job_description' (remove sentences like \"Remove photo First name Last name Email Password ( 8 + characters )\");\n",
    "3. Clean up the 'level' column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebb0f88-0ac8-475a-8915-015f2ef18641",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b2c32c38-1c90-4dca-b20a-e6500d64c284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_Job_description(text):\n",
    "    senetences_to_remove = [\"Remove photo First name Last name Email Password (8+ characters) \",\n",
    "                            \"By clicking Agree & Join\",\n",
    "                            \"you agree to the LinkedIn User Agreement\",\n",
    "                            \"Privacy Policy and Cookie Policy\",\n",
    "                            \"Continue Agree & Join or Apply on company website\",\n",
    "                            \"Security verification\",\n",
    "                            \"Close Already on LinkedIn ?\",\n",
    "                            \"Close Already on LinkedIn?\",\n",
    "                            \"Sign in Save Save job Save this job with your existing LinkedIn profile , or create a new one\",\n",
    "                            \"Sign in Save Save job Save this job with your existing LinkedIn profile, or create a new one\",\n",
    "                            \"Your job seeking activity is only visible to you\",\n",
    "                            \"Email Continue Welcome back\"]\n",
    "    for sentence in senetences_to_remove:\n",
    "        result = text.find(sentence)\n",
    "        if result>-1:\n",
    "            text = text[:result] + text[result+len(sentence):] # remove sentence from text\n",
    "\n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec0f9a2c-ddd6-428a-82e5-62e3b9d061b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posted_date(posted_time_ago,date_scraping):\n",
    "    \"\"\"Convert posted_time_ago to number of days.\n",
    "    For example, 1 month ago is replaced by 30. 1 week by 7 and so on...\"\"\"\n",
    "    posted_date = None\n",
    "    \n",
    "    try:\n",
    "        details = posted_time_ago.split()\n",
    "        N_DAYS_AGO = int(details[0])\n",
    "        day_week_month_year = details[1] \n",
    "        if day_week_month_year.startswith(\"day\"):\n",
    "            N_DAYS_AGO = N_DAYS_AGO\n",
    "        elif day_week_month_year.startswith(\"week\"):\n",
    "            N_DAYS_AGO = N_DAYS_AGO*7\n",
    "        elif day_week_month_year.startswith(\"month\"):\n",
    "            N_DAYS_AGO = N_DAYS_AGO*30\n",
    "        elif day_week_month_year.startswith(\"year\"):\n",
    "            N_DAYS_AGO = N_DAYS_AGO*365\n",
    "        else:\n",
    "            N_DAYS_AGO = None\n",
    "\n",
    "        posted_date = date_scraping - datetime.timedelta(days=N_DAYS_AGO)\n",
    "    except:\n",
    "        posted_date = None\n",
    "\n",
    "    return posted_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "16261cc5-a680-4d45-8727-596f4b312022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_ID</th>\n",
       "      <th>Job_txt</th>\n",
       "      <th>company</th>\n",
       "      <th>job-title</th>\n",
       "      <th>level</th>\n",
       "      <th>location</th>\n",
       "      <th>posted-time-ago</th>\n",
       "      <th>nb_candidats</th>\n",
       "      <th>scraping_date</th>\n",
       "      <th>posted_date</th>\n",
       "      <th>skills</th>\n",
       "      <th>match_score</th>\n",
       "      <th>missing_skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3766879037</td>\n",
       "      <td>Scientifique des donn√©es- Op√©rations Ubisoft M...</td>\n",
       "      <td>Ubisoft</td>\n",
       "      <td>Scientifique des donn√©es- Op√©rations</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>Montreal, Quebec, Canada</td>\n",
       "      <td>2 days ago</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2023-11-25</td>\n",
       "      <td>2023-11-23</td>\n",
       "      <td>[python, marketing, security, sql, business, d...</td>\n",
       "      <td>75.0</td>\n",
       "      <td>security,collaboration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>3762695166</td>\n",
       "      <td>Directeur.trice du d√©partement science des don...</td>\n",
       "      <td>ChrysaLabs</td>\n",
       "      <td>Directeur.trice du d√©partement science des don...</td>\n",
       "      <td>Director</td>\n",
       "      <td>Montreal, Quebec, Canada</td>\n",
       "      <td>4 days ago</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2023-11-25</td>\n",
       "      <td>2023-11-21</td>\n",
       "      <td>[python, sql, collaboration, marketing]</td>\n",
       "      <td>75.0</td>\n",
       "      <td>collaboration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>3766203603</td>\n",
       "      <td>Director, Client Partner- EN RBC Montreal, Que...</td>\n",
       "      <td>RBC</td>\n",
       "      <td>Director, Client Partner- EN</td>\n",
       "      <td>Not Applicable</td>\n",
       "      <td>Montreal, Quebec, Canada</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-11-25</td>\n",
       "      <td>NaT</td>\n",
       "      <td>[analytics, business, security, support]</td>\n",
       "      <td>75.0</td>\n",
       "      <td>security</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3766874608</td>\n",
       "      <td>Data Scientist Ubisoft Montreal, Quebec, Canad...</td>\n",
       "      <td>Ubisoft</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>Montreal, Quebec, Canada</td>\n",
       "      <td>2 days ago</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2023-11-25</td>\n",
       "      <td>2023-11-23</td>\n",
       "      <td>[database, machine learning, databases, market...</td>\n",
       "      <td>69.2</td>\n",
       "      <td>databases,security,visualization,segment,software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>3767691220</td>\n",
       "      <td>Data Analyst Logikk Montreal, Quebec, Canada 1...</td>\n",
       "      <td>Logikk</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>Montreal, Quebec, Canada</td>\n",
       "      <td>1 day ago</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-11-25</td>\n",
       "      <td>2023-11-24</td>\n",
       "      <td>[python, machine learning, analytics, computer...</td>\n",
       "      <td>66.7</td>\n",
       "      <td>computer vision,play,ai</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Job_ID                                            Job_txt  \\\n",
       "13   3766879037  Scientifique des donn√©es- Op√©rations Ubisoft M...   \n",
       "207  3762695166  Directeur.trice du d√©partement science des don...   \n",
       "230  3766203603  Director, Client Partner- EN RBC Montreal, Que...   \n",
       "3    3766874608  Data Scientist Ubisoft Montreal, Quebec, Canad...   \n",
       "103  3767691220  Data Analyst Logikk Montreal, Quebec, Canada 1...   \n",
       "\n",
       "        company                                          job-title  \\\n",
       "13      Ubisoft               Scientifique des donn√©es- Op√©rations   \n",
       "207  ChrysaLabs  Directeur.trice du d√©partement science des don...   \n",
       "230         RBC                       Director, Client Partner- EN   \n",
       "3       Ubisoft                                     Data Scientist   \n",
       "103      Logikk                                       Data Analyst   \n",
       "\n",
       "                level                  location posted-time-ago  nb_candidats  \\\n",
       "13   Mid-Senior level  Montreal, Quebec, Canada      2 days ago          29.0   \n",
       "207          Director  Montreal, Quebec, Canada      4 days ago          25.0   \n",
       "230    Not Applicable  Montreal, Quebec, Canada            None           NaN   \n",
       "3    Mid-Senior level  Montreal, Quebec, Canada      2 days ago          96.0   \n",
       "103  Mid-Senior level  Montreal, Quebec, Canada       1 day ago           NaN   \n",
       "\n",
       "    scraping_date posted_date  \\\n",
       "13     2023-11-25  2023-11-23   \n",
       "207    2023-11-25  2023-11-21   \n",
       "230    2023-11-25         NaT   \n",
       "3      2023-11-25  2023-11-23   \n",
       "103    2023-11-25  2023-11-24   \n",
       "\n",
       "                                                skills  match_score  \\\n",
       "13   [python, marketing, security, sql, business, d...         75.0   \n",
       "207            [python, sql, collaboration, marketing]         75.0   \n",
       "230           [analytics, business, security, support]         75.0   \n",
       "3    [database, machine learning, databases, market...         69.2   \n",
       "103  [python, machine learning, analytics, computer...         66.7   \n",
       "\n",
       "                                        missing_skills  \n",
       "13                              security,collaboration  \n",
       "207                                      collaboration  \n",
       "230                                           security  \n",
       "3    databases,security,visualization,segment,software  \n",
       "103                            computer vision,play,ai  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs_DF['scraping_date'] = pd.to_datetime(datetime.date.today())\n",
    "jobs_DF['posted_date'] = np.vectorize(get_posted_date)(jobs_DF['posted-time-ago'], jobs_DF['scraping_date'])\n",
    "\n",
    "jobs_DF['Job_txt'] = jobs_DF['Job_txt'].apply(clean_Job_description)\n",
    "jobs_DF.level = jobs_DF.level.apply(lambda x:x.replace(\"Employment type\\n        \\n\\n          \",\"\") if x is not None else x)\n",
    "\n",
    "jobs_DF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3bec7f-929b-4ec6-a1ef-3b5b34018515",
   "metadata": {},
   "source": [
    "## Save to json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "45a01750-e57b-45df-95e2-757649ec5707",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_DF.to_json(\"../data/linkedin_jobs_scraped.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
